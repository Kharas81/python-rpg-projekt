{
  // Globale Konfigurationseinstellungen für das RPG-Projekt

  "logging": {
    // Konfiguration für das Logging-System (siehe src/utils/logging_setup.py)
    "log_level": "INFO", // Mögliche Werte: "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
    "log_to_console": true, // Ob Logs auch auf der Konsole ausgegeben werden sollen
    "log_to_file": true, // Ob Logs in eine Datei geschrieben werden sollen
    "log_file": "logs/rpg_game.log", // Pfad zur Log-Datei (relativ zum Projekt-Root)
                                      // Das 'logs' Verzeichnis muss existieren (wird vom Logger erstellt)
                                      // und ist in .gitignore
    "log_format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s", // Format der Log-Einträge
    "log_date_format": "%Y-%m-%d %H:%M:%S" // Datumsformat für Log-Einträge
  },

  "game_settings": {
    // Allgemeine Spiel-Einstellungen (Beispiele, können erweitert werden)
    "default_player_class": "krieger", // Standard-Spielerklasse, falls keine gewählt
    "min_damage": 1, // Minimaler Schaden, der immer verursacht wird
    "base_weapon_damage": 5, // Standard-Waffenschaden, wenn keine Waffe spezifiziert (für Schadensberechnung)
    "hit_chance_base": 90, // Basis-Trefferchance (siehe ANNEX_GAME_DEFINITIONS_SUMMARY.md)
    "hit_chance_accuracy_factor": 3, // Faktor für Genauigkeit bei Trefferchance
    "hit_chance_evasion_factor": 2, // Faktor für Ausweichen bei Trefferchance
    "hit_chance_min": 5, // Minimale Trefferchance in %
    "hit_chance_max": 95, // Maximale Trefferchance in %
    "xp_level_base": 100, // Basis-XP für Levelaufstieg
    "xp_level_factor": 1.5 // Faktor für XP-Steigerung pro Level
  },

  "rl_settings": {
    // Einstellungen für Reinforcement Learning (Platzhalter für später)
    "model_save_path": "src/ai/models/", // Verzeichnis zum Speichern trainierter Modelle
    "log_path": "logs/", // Verzeichnis für RL-Trainingslogs (TensorBoard etc.)
    "training_timesteps": 100000, // Beispiel: Anzahl der Trainingsschritte
    "evaluation_episodes": 10, // Beispiel: Anzahl der Episoden zur Evaluierung
    "ppo_config": { // Beispiel für spezifische RL-Algorithmus-Parameter
        "learning_rate": 0.0003,
        "n_steps": 2048,
        "batch_size": 64,
        "n_epochs": 10,
        "gamma": 0.99,
        "gae_lambda": 0.95,
        "clip_range": 0.2,
        "ent_coef": 0.0,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
    }
  },

  "paths": {
      // Zentrale Pfaddefinitionen (relativ zum Projekt-Root)
      "definitions": "src/definitions/json_data",
      "logs": "logs",
      "reports": "reports",
      "saves": "saves", // Optional für Spielstände
      "rl_models": "src/ai/models"
  }

} // Ende des Haupt-Objekts
